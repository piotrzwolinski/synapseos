"""
Evaluation Script for GraphRAG Sales Assistant
Tests system responses against expected answers using LLM evaluation.
"""

import time
import json
import httpx
from dataclasses import dataclass
from typing import Optional
from google import genai
from google.genai import types
import os
from dotenv import load_dotenv
from tabulate import tabulate

load_dotenv(dotenv_path="../.env")

# Initialize Gemini client for evaluation
client = genai.Client(api_key=os.getenv("GEMINI_API_KEY"))

# =============================================================================
# TEST CASES
# =============================================================================

@dataclass
class TestCase:
    id: str
    question: str
    expected_behavior: str
    key_checks: list[str]  # What the evaluator should look for


TEST_CASES = [
    TestCase(
        id="Q1",
        question="Potrzebujƒô taniej obudowy GDB w standardowym ocynku do wentylacji hali basenowej. Czy to dobry wyb√≥r?",
        expected_behavior="OSTRZE≈ªENIE/BLOKADA - Basen=Chlor/Korozja wymaga C4/C5. Ocynk (FZ) to klasa C3. Rekomendacja: ZM lub RF.",
        key_checks=[
            "Wykrycie ryzyka korozji (chlor/basen)",
            "Ostrze≈ºenie o nieodpowiednim materiale (FZ/ocynk)",
            "Rekomendacja materia≈Çu odpornego na korozjƒô (ZM, RF, stal nierdzewna)",
            "Wskazanie klasy korozyjno≈õci (C3 vs C4/C5)"
        ]
    ),
    TestCase(
        id="Q2",
        question="Mamy problem z zapachem spalin na parkingu podziemnym. Jaki model workowy (GDB) polecacie?",
        expected_behavior="KOREKTA SERII - GDB (workowy) do py≈Ç√≥w, nie gaz√≥w. Dla zapachu/spalin potrzeba filtracji molekularnej (wƒôgiel) = seria GDC.",
        key_checks=[
            "Rozpoznanie ≈ºe GDB nie nadaje siƒô do gaz√≥w/zapach√≥w",
            "Wskazanie ≈ºe filtr workowy jest do py≈Ç√≥w",
            "Rekomendacja serii GDC (wƒôglowa/patronowa)",
            "Wyja≈õnienie r√≥≈ºnicy miƒôdzy filtracjƒÖ czƒÖsteczkowƒÖ a molekularnƒÖ"
        ]
    ),
    TestCase(
        id="Q3",
        question="Muszƒô wstawiƒá filtr wƒôglowy w istniejƒÖcy kana≈Ç. Mam nietypowƒÖ lukƒô monta≈ºowƒÖ dok≈Çadnie 1050 mm. Co tam wejdzie?",
        expected_behavior="DOB√ìR WERSJI FLEX - Nietypowy wymiar wymaga wersji z regulowanƒÖ d≈Çugo≈õciƒÖ. Rekomendacja: GDMI FLEX lub GDC FLEX.",
        key_checks=[
            "Rozpoznanie potrzeby retrofitu/nietypowego wymiaru",
            "Rekomendacja wersji FLEX z regulowanƒÖ d≈Çugo≈õciƒÖ",
            "Podanie zakresu regulacji (850-1100mm lub podobny)",
            "Wskazanie konkretnego modelu FLEX"
        ]
    ),
    TestCase(
        id="Q4",
        question="Projektujƒô centralƒô na dachu w strefie zimnej. Czy model GDB-600x600 bƒôdzie odpowiedni?",
        expected_behavior="WYM√ìG IZOLACJI - Dach/Zimno = ryzyko kondensacji. GDB jest nieizolowane. Rekomendacja: seria GDMI (izolowana).",
        key_checks=[
            "Wykrycie ryzyka kondensacji (dach + zimno)",
            "Informacja ≈ºe GDB jest nieizolowane",
            "Rekomendacja serii GDMI (izolowana termicznie)",
            "Wyja≈õnienie problemu kondensacji"
        ]
    ),
    TestCase(
        id="Q5",
        question="Chcƒô zam√≥wiƒá obudowƒô GDC o d≈Çugo≈õci 750 mm i do≈Ço≈ºyƒá do niej szynƒô na filtr doczyszczajƒÖcy (polisfiltr). Poproszƒô kod.",
        expected_behavior="KONFLIKT KONFIGURACJI - Opcja 'Polis' wymaga min. 900mm d≈Çugo≈õci. Rekomendacja: zmiana d≈Çugo≈õci na 900mm.",
        key_checks=[
            "Wykrycie konfliktu konfiguracji",
            "Informacja o wymaganiach opcji Polis (wymaga wiƒôkszej d≈Çugo≈õci)",
            "Rekomendacja zmiany d≈Çugo≈õci na 900mm lub wiƒôcej",
            "Wyja≈õnienie ogranicze≈Ñ konfiguracyjnych"
        ]
    ),
    TestCase(
        id="Q6",
        question="Potrzebujƒô filtra do hali produkcyjnej. Jaki polecacie?",
        expected_behavior="KLARYFIKACJA - Brak kluczowych parametr√≥w. System powinien zapytaƒá o: typ zanieczyszczenia (py≈Ç/gaz), przep≈Çyw, ≈õrodowisko.",
        key_checks=[
            "Wykrycie braku kluczowych parametr√≥w",
            "Zapytanie o typ zanieczyszczenia (py≈Ç vs gaz)",
            "Zapytanie o wymagany przep≈Çyw powietrza",
            "Opcjonalnie: zapytanie o ≈õrodowisko/aplikacjƒô"
        ]
    ),
    # =========================================================================
    # COMPLEX FILTER HOUSING SELECTION TEST CASES (Q7-Q11)
    # =========================================================================
    TestCase(
        id="Q7",
        question="Potrzebujƒô obudowy na filtr wƒôglowy do kuchni przemys≈Çowej. Przep≈Çyw 4500 m¬≥/h, ale mam tylko 700mm przestrzeni monta≈ºowej na d≈Çugo≈õƒá. Budget jest ograniczony.",
        expected_behavior="KONFLIKT WYMIAROWY + DOB√ìR - 4500 m¬≥/h wymaga wiƒôkszej obudowy (min. GDC-900x600). 700mm to za ma≈Ço. System powinien: (1) wskazaƒá konflikt przestrzeni, (2) zaproponowaƒá alternatywy: mniejszy przep≈Çyw lub uk≈Çad 2x mniejsze jednostki r√≥wnolegle.",
        key_checks=[
            "Rozpoznanie konfliktu: wymagana wydajno≈õƒá vs dostƒôpna przestrze≈Ñ",
            "Informacja ≈ºe 4500 m¬≥/h wymaga obudowy min. 900mm d≈Çugo≈õci",
            "Propozycja alternatyw: redukcja przep≈Çywu LUB uk≈Çad r√≥wnoleg≈Çy",
            "Wskazanie ≈ºe kuchnia wymaga filtracji t≈Çuszcz√≥w przed wƒôglem (pre-filter)",
            "Uwzglƒôdnienie bud≈ºetu w rekomendacji"
        ]
    ),
    TestCase(
        id="Q8",
        question="Szukam najta≈Ñszej obudowy GDC do filtracji zapach√≥w z lakierni. Wystarczy standardowy ocynk. Przep≈Çyw oko≈Ço 2000 m¬≥/h.",
        expected_behavior="OSTRZE≈ªENIE CHEMICZNE - Lakiernia = rozpuszczalniki, LZO (VOC). Agresywne ≈õrodowisko wymaga odpornych materia≈Ç√≥w. Standardowy ocynk mo≈ºe korodowaƒá. Rekomendacja: min. ZM lub pow≈Çoka chemoodporna + wƒôgiel aktywny dedykowany do VOC.",
        key_checks=[
            "Wykrycie agresywnego ≈õrodowiska chemicznego (rozpuszczalniki/VOC)",
            "Ostrze≈ºenie o nieodpowiednim materiale (ocynk) do lakierni",
            "Rekomendacja materia≈Çu chemoodpornego (ZM, RF, pow≈Çoka)",
            "Informacja o potrzebie wƒôgla dedykowanego do VOC (nie zwyk≈Çy wƒôgiel)",
            "Wyja≈õnienie ryzyka korozji chemicznej"
        ]
    ),
    TestCase(
        id="Q9",
        question="Mam stacjƒô obs≈Çugi samochod√≥w - na warsztacie py≈Ç z szlifowania karoserii, a przy wje≈∫dzie spaliny z silnik√≥w. Czy jedna obudowa za≈Çatwi sprawƒô? Przep≈Çyw razem jakie≈õ 3000 m¬≥/h.",
        expected_behavior="WYMAGANE DWA TYPY FILTRACJI - Py≈Ç (czƒÖstki) wymaga filtra workowego/kasetowego, spaliny (gazy) wymagajƒÖ wƒôgla aktywnego. Jedna obudowa workowa NIE usunie spalin. Rekomendacja: osobne systemy LUB obudowa kombinowana z pre-filtrem + wƒôglem.",
        key_checks=[
            "Rozpoznanie dw√≥ch r√≥≈ºnych typ√≥w zanieczyszcze≈Ñ (py≈Ç + gazy)",
            "Wyja≈õnienie ≈ºe filtr workowy nie usuwa gaz√≥w",
            "Wyja≈õnienie ≈ºe filtr wƒôglowy nie nadaje siƒô do du≈ºych ilo≈õci py≈Çu",
            "Propozycja rozwiƒÖzania: osobne systemy LUB kombinowany (pre-filtr + wƒôgiel)",
            "Prawid≈Çowy dob√≥r wydajno≈õci dla obu zastosowa≈Ñ"
        ]
    ),
    TestCase(
        id="Q10",
        question="Wymieniamy stary filtr w istniejƒÖcej instalacji. Otw√≥r w kanale ma wymiary 580x580mm (ko≈Çnierz). Potrzebujemy filtracji wƒôglowej na zapachy z gastronomii. Co pasuje?",
        expected_behavior="DOB√ìR RETROFIT - Niestandardowy wymiar 580x580 nie pasuje do standardowych modu≈Ç√≥w (300/600/900). Rekomendacja: (1) adapter/przej≈õci√≥wka na 600x600, lub (2) wersja FLEX z regulacjƒÖ, lub (3) obudowa na zam√≥wienie.",
        key_checks=[
            "Rozpoznanie niestandardowego wymiaru (580 ‚â† modu≈Çy 600)",
            "Informacja o standardowych modu≈Çach wymiarowych (300/600/900)",
            "Propozycja adaptera/przej≈õci√≥wki jako rozwiƒÖzania",
            "Alternatywnie: propozycja wersji FLEX lub custom",
            "Dob√≥r typu filtracji do gastronomii (wƒôgiel + pre-filtr t≈Çuszczowy)"
        ]
    ),
    TestCase(
        id="Q11",
        question="Projektujƒô wentylacjƒô dla ch≈Çodni (-25¬∞C) z czƒôstym otwieraniem wr√≥t (du≈ºe r√≥≈ºnice temperatur). Potrzebujƒô filtracji py≈Çu. Kt√≥ry model obudowy bƒôdzie najlepszy?",
        expected_behavior="WYMOGI TERMICZNE EKSTREMALNE - Niska temperatura + szok termiczny = ryzyko kondensacji, szronu, uszkodzenia materia≈Çu. Wymagania: (1) izolacja termiczna (GDMI), (2) materia≈Ç odporny na niskie temp, (3) mo≈ºliwy grza≈Çka antykondensacyjna.",
        key_checks=[
            "Wykrycie ekstremalnych warunk√≥w termicznych (-25¬∞C)",
            "Identyfikacja ryzyka kondensacji przy r√≥≈ºnicach temperatur",
            "Rekomendacja obudowy izolowanej (seria GDMI)",
            "Informacja o materia≈Çach odpornych na mr√≥z",
            "Opcjonalnie: sugestia grza≈Çki antykondensacyjnej lub klapy przeciwszronowej"
        ]
    ),
]

# =============================================================================
# EVALUATION PROMPT
# =============================================================================

EVALUATION_PROMPT = """Jeste≈õ ewaluatorem systemu AI do wspomagania sprzeda≈ºy in≈ºynierskiej.

## OCZEKIWANE ZACHOWANIE SYSTEMU
{expected_behavior}

## KLUCZOWE ELEMENTY DO SPRAWDZENIA
{key_checks}

## ODPOWIED≈π SYSTEMU DO OCENY
{system_response}

## INSTRUKCJE OCENY
Oce≈Ñ odpowied≈∫ systemu w skali 0-10 dla ka≈ºdego kryterium:

1. **Wykrycie Ryzyka (0-10)**: Czy system poprawnie zidentyfikowa≈Ç potencjalny problem/ryzyko?
2. **Trafno≈õƒá Rekomendacji (0-10)**: Czy rekomendacja jest technicznie poprawna?
3. **Kompletno≈õƒá (0-10)**: Czy odpowied≈∫ zawiera wszystkie kluczowe elementy?
4. **Jasno≈õƒá (0-10)**: Czy odpowied≈∫ jest zrozumia≈Ça i dobrze ustrukturyzowana?

## FORMAT ODPOWIEDZI (JSON)
{{
  "risk_detection": <0-10>,
  "recommendation_accuracy": <0-10>,
  "completeness": <0-10>,
  "clarity": <0-10>,
  "overall_score": <0-10>,
  "detected_issues": ["lista wykrytych przez system problem√≥w"],
  "missing_elements": ["lista brakujƒÖcych element√≥w"],
  "comment": "kr√≥tki komentarz do oceny"
}}

Zwr√≥ƒá TYLKO valid JSON."""

# =============================================================================
# API CALLS
# =============================================================================

BASE_URL = "http://localhost:8000"



async def query_system(question: str) -> tuple[dict, float]:
    """Query the deep-explainable endpoint and return response + time."""
    async with httpx.AsyncClient() as client:
        start_time = time.time()
        try:
            response = await client.post(
                f"{BASE_URL}/consult/deep-explainable",
                json={"query": question},
                timeout=120.0
            )
            elapsed = time.time() - start_time

            if response.status_code == 200:
                return response.json(), elapsed
            else:
                return {"error": f"HTTP {response.status_code}"}, elapsed
        except Exception as e:
            elapsed = time.time() - start_time
            return {"error": str(e)}, elapsed


def evaluate_response(test_case: TestCase, system_response: dict) -> dict:
    """Use LLM to evaluate the system response."""

    # Extract content from response
    if "error" in system_response:
        return {
            "risk_detection": 0,
            "recommendation_accuracy": 0,
            "completeness": 0,
            "clarity": 0,
            "overall_score": 0,
            "detected_issues": [],
            "missing_elements": ["System error"],
            "comment": f"Error: {system_response['error']}"
        }

    # Build response text from segments
    content_text = ""
    if "content_segments" in system_response:
        content_text = "".join(seg.get("text", "") for seg in system_response["content_segments"])

    # Add reasoning summary
    reasoning_text = ""
    if "reasoning_summary" in system_response:
        reasoning_text = "\n".join(
            f"- {step.get('step', '')}: {step.get('description', '')}"
            for step in system_response["reasoning_summary"]
        )

    # Add warnings
    warnings_text = ""
    if system_response.get("policy_warnings"):
        warnings_text = "\nOstrze≈ºenia: " + "; ".join(system_response["policy_warnings"])

    # Add risk detection flag
    risk_flag = ""
    if system_response.get("risk_detected"):
        risk_flag = "\n[SYSTEM WYKRY≈Å RYZYKO IN≈ªYNIERYJNE]"

    # Add clarification flag
    clarification_flag = ""
    if system_response.get("clarification_needed"):
        clarification_flag = "\n[SYSTEM WYMAGA KLARYFIKACJI]"
        if system_response.get("clarification"):
            clar = system_response["clarification"]
            clarification_flag += f"\nBrakuje: {clar.get('missing_info', 'N/A')}"
            clarification_flag += f"\nDlaczego: {clar.get('why_needed', 'N/A')}"
            clarification_flag += f"\nPytanie: {clar.get('question', 'N/A')}"

    full_response = f"""
REASONING:
{reasoning_text}

ODPOWIED≈π:
{content_text}
{warnings_text}
{risk_flag}
{clarification_flag}

PRODUCT CARD:
{json.dumps(system_response.get('product_card'), ensure_ascii=False, indent=2) if system_response.get('product_card') else 'Brak'}

STATYSTYKI:
- Graph Facts: {system_response.get('graph_facts_count', 0)}
- Inferences: {system_response.get('inference_count', 0)}
- Confidence: {system_response.get('confidence_level', 'unknown')}
- Clarification Needed: {system_response.get('clarification_needed', False)}
"""

    # Build evaluation prompt
    key_checks_formatted = "\n".join(f"- {check}" for check in test_case.key_checks)

    prompt = EVALUATION_PROMPT.format(
        expected_behavior=test_case.expected_behavior,
        key_checks=key_checks_formatted,
        system_response=full_response
    )

    try:
        response = client.models.generate_content(
            model="gemini-2.0-flash",
            contents=[
                types.Content(
                    role="user",
                    parts=[types.Part.from_text(text=prompt)]
                )
            ],
            config=types.GenerateContentConfig(
                response_mime_type="application/json",
            ),
        )
        return json.loads(response.text)
    except Exception as e:
        return {
            "risk_detection": 0,
            "recommendation_accuracy": 0,
            "completeness": 0,
            "clarity": 0,
            "overall_score": 0,
            "detected_issues": [],
            "missing_elements": ["Evaluation error"],
            "comment": f"Evaluation error: {str(e)}"
        }


# =============================================================================
# MAIN EVALUATION
# =============================================================================

async def run_evaluation():
    """Run full evaluation across models and thinking levels."""

    # Configuration combinations
    MODELS = [
        ("gemini-3-pro-preview", "Pro"),
        ("gemini-3-flash-preview", "Flash"),
    ]
    THINKING_LEVELS = [
        ("low", "Low"),
        ("high", "High"),
    ]

    # Results storage
    results = []

    print("=" * 80)
    print("GRAPHRAG SALES ASSISTANT - EVALUATION")
    print("=" * 80)
    print(f"\nRunning {len(TEST_CASES)} test cases x {len(MODELS)} models x {len(THINKING_LEVELS)} thinking levels")
    print(f"Total tests: {len(TEST_CASES) * len(MODELS) * len(THINKING_LEVELS)}")
    print()

    for model_id, model_name in MODELS:
        for thinking_id, thinking_name in THINKING_LEVELS:
            config_name = f"{model_name}/{thinking_name}"
            print(f"\n{'='*60}")
            print(f"Configuration: {config_name}")
            print(f"{'='*60}")


            for test_case in TEST_CASES:
                print(f"\n  [{test_case.id}] {test_case.question[:50]}...")

                # Query system
                response, elapsed = await query_system(test_case.question)
                print(f"      Response time: {elapsed:.2f}s")

                # Evaluate
                evaluation = evaluate_response(test_case, response)
                print(f"      Score: {evaluation.get('overall_score', 0)}/10")

                results.append({
                    "test_id": test_case.id,
                    "question": test_case.question[:40] + "...",
                    "config": config_name,
                    "model": model_name,
                    "thinking": thinking_name,
                    "time_s": round(elapsed, 2),
                    "risk_detection": evaluation.get("risk_detection", 0),
                    "recommendation": evaluation.get("recommendation_accuracy", 0),
                    "completeness": evaluation.get("completeness", 0),
                    "clarity": evaluation.get("clarity", 0),
                    "overall": evaluation.get("overall_score", 0),
                    "comment": evaluation.get("comment", "")[:50],
                    "risk_detected": response.get("risk_detected", False),
                    "clarification_needed": response.get("clarification_needed", False),
                })

    return results


def generate_results_table(results: list[dict]) -> str:
    """Generate a formatted results table."""

    # Pivot table: Questions as rows, Config combinations as columns
    configs = sorted(set(r["config"] for r in results))
    test_ids = sorted(set(r["test_id"] for r in results), key=lambda x: int(x[1:]))

    # Build header
    headers = ["Test", "Question"]
    for config in configs:
        headers.extend([f"{config}\nScore", f"{config}\nTime(s)"])

    # Build rows
    rows = []
    for test_id in test_ids:
        test_results = [r for r in results if r["test_id"] == test_id]
        question = test_results[0]["question"] if test_results else ""

        row = [test_id, question]
        for config in configs:
            config_result = next((r for r in test_results if r["config"] == config), None)
            if config_result:
                score = config_result["overall"]
                time_s = config_result["time_s"]
                # Add risk/clarification indicators
                indicator = ""
                if config_result.get("risk_detected"):
                    indicator = "üõ°Ô∏è"
                elif config_result.get("clarification_needed"):
                    indicator = "‚ùì"
                row.extend([f"{score}/10 {indicator}", f"{time_s}s"])
            else:
                row.extend(["N/A", "N/A"])

        rows.append(row)

    # Add summary row
    summary_row = ["", "≈öREDNIA"]
    for config in configs:
        config_results = [r for r in results if r["config"] == config]
        avg_score = sum(r["overall"] for r in config_results) / len(config_results) if config_results else 0
        avg_time = sum(r["time_s"] for r in config_results) / len(config_results) if config_results else 0
        summary_row.extend([f"{avg_score:.1f}/10", f"{avg_time:.1f}s"])
    rows.append(summary_row)

    return tabulate(rows, headers=headers, tablefmt="grid")


def generate_detailed_report(results: list[dict]) -> str:
    """Generate a detailed evaluation report."""

    report = []
    report.append("\n" + "=" * 80)
    report.append("SZCZEG√ì≈ÅOWY RAPORT EWALUACJI")
    report.append("=" * 80)

    for test_id in sorted(set(r["test_id"] for r in results), key=lambda x: int(x[1:])):
        test_results = [r for r in results if r["test_id"] == test_id]
        if not test_results:
            continue

        question = next((tc.question for tc in TEST_CASES if tc.id == test_id), "")
        expected = next((tc.expected_behavior for tc in TEST_CASES if tc.id == test_id), "")

        report.append(f"\n{'‚îÄ'*80}")
        report.append(f"[{test_id}] {question}")
        report.append(f"{'‚îÄ'*80}")
        report.append(f"OCZEKIWANE: {expected}")
        report.append("")

        for r in test_results:
            risk_flag = "üõ°Ô∏è RYZYKO" if r.get("risk_detected") else ""
            report.append(f"  {r['config']:15} | Score: {r['overall']:2}/10 | Time: {r['time_s']:5.1f}s | {risk_flag}")
            report.append(f"                    | Ryzyko: {r['risk_detection']}/10 | Rekomendacja: {r['recommendation']}/10 | Kompletno≈õƒá: {r['completeness']}/10")
            if r.get("comment"):
                report.append(f"                    | Komentarz: {r['comment']}")

    return "\n".join(report)


async def main():
    """Main entry point."""
    import sys

    print("\nStarting evaluation...\n")

    try:
        results = await run_evaluation()
    except Exception as e:
        print(f"\nError during evaluation: {e}")
        sys.exit(1)

    # Generate and print results table
    print("\n" + "=" * 80)
    print("TABELA WYNIK√ìW")
    print("=" * 80)
    print(generate_results_table(results))

    # Generate detailed report
    print(generate_detailed_report(results))

    # Save results to JSON
    output_file = "evaluation_results.json"
    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    print(f"\nWyniki zapisane do: {output_file}")

    # Summary
    print("\n" + "=" * 80)
    print("PODSUMOWANIE")
    print("=" * 80)

    configs = sorted(set(r["config"] for r in results))
    for config in configs:
        config_results = [r for r in results if r["config"] == config]
        avg_score = sum(r["overall"] for r in config_results) / len(config_results)
        avg_time = sum(r["time_s"] for r in config_results) / len(config_results)
        risk_detected_count = sum(1 for r in config_results if r.get("risk_detected"))

        print(f"{config:20} | Avg Score: {avg_score:.1f}/10 | Avg Time: {avg_time:.1f}s | Risk Detected: {risk_detected_count}/{len(config_results)}")


if __name__ == "__main__":
    import asyncio
    asyncio.run(main())
